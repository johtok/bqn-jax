{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01995b66",
   "metadata": {},
   "source": [
    "# MLP in BQN on JAX — Polynomial Regression\n",
    "\n",
    "This notebook is an interactive version of `scripts/simple_mlp_jaxbqn.py`.  \n",
    "It trains a small MLP (1 → 16 → 1) expressed entirely in BQN, compiled to\n",
    "JAX, and optimized with an Adam optimizer also written in BQN.\n",
    "\n",
    "**All math lives in nine BQN expressions** — JAX supplies the\n",
    "differentiation, JIT compilation, and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d0c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    for candidate in (start, *start.parents):\n",
    "        if (candidate / \"pyproject.toml\").exists() and (candidate / \"src\" / \"bqn_jax\").exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(\"Could not locate the bqn-jax repo root.\")\n",
    "\n",
    "_repo_root = _find_repo_root(Path.cwd())\n",
    "_src = _repo_root / \"src\"\n",
    "if str(_src) not in sys.path:\n",
    "    sys.path.insert(0, str(_src))\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bqn_jax import ShapePolicy, compile_expression\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08108a8d",
   "metadata": {},
   "source": [
    "## The Nine BQN Expressions\n",
    "\n",
    "| # | Purpose | BQN |\n",
    "|---|---------|-----|\n",
    "| E0 | Forward pass (dense + ReLU-like + output) | `+´(v×÷⟜(1˙⊸+∘\\|)(w(+´∘×)x+b))+bo` |\n",
    "| E1 | Target polynomial | `((0.2+(0.9×(+´x)))-(0.3×((+´x)⋆2)))+(0.4×((+´x)⋆3))` |\n",
    "| E2 | MSE loss | `+´×˜(p-y)÷≠y` |\n",
    "| E3 | RMSE | `√(+´×˜(p-y)÷≠y)` |\n",
    "| E4 | Adam momentum update | `(beta1×m)+((1-beta1)×g)` |\n",
    "| E5 | Adam velocity update | `(beta2×v)+((1-beta2)××˜g)` |\n",
    "| E6 | Bias-corrected momentum | `m÷(1-(beta1⋆t))` |\n",
    "| E7 | Bias-corrected velocity | `v÷(1-(beta2⋆t))` |\n",
    "| E8 | Parameter update | `p-(lr×mhat÷((√vhat)+eps))` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082498d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nine BQN expressions define the entire MLP + optimizer:\n",
      "  E0 = +´(v×÷⟜(1˙⊸+∘|)(w(+´∘×)x+b))+bo\n",
      "  E1 = ((0.2 + (0.9 × (+´x))) - (0.3 × ((+´x) ⋆ 2))) + (0.4 × ((+´x) ⋆ 3))\n",
      "  E2 = +´×˜(p-y)÷≠y\n",
      "  E3 = √(+´×˜(p-y)÷≠y)\n",
      "  E4 = (beta1×m)+((1-beta1)×g)\n",
      "  E5 = (beta2×v)+((1-beta2)××˜g)\n",
      "  E6 = m÷(1-(beta1⋆t))\n",
      "  E7 = v÷(1-(beta2⋆t))\n",
      "  E8 = p-(lr×mhat÷((√vhat)+eps))\n"
     ]
    }
   ],
   "source": [
    "# ── BQN source expressions ──\n",
    "E0 = \"+´(v×÷⟜(1˙⊸+∘|)(w(+´∘×)x+b))+bo\"       # forward pass\n",
    "E1 = \"((0.2 + (0.9 × (+´x))) - (0.3 × ((+´x) ⋆ 2))) + (0.4 × ((+´x) ⋆ 3))\"  # target\n",
    "E2 = \"+´×˜(p-y)÷≠y\"                              # MSE\n",
    "E3 = \"√(+´×˜(p-y)÷≠y)\"                           # RMSE\n",
    "E4 = \"(beta1×m)+((1-beta1)×g)\"                    # Adam m-update\n",
    "E5 = \"(beta2×v)+((1-beta2)××˜g)\"                  # Adam v-update\n",
    "E6 = \"m÷(1-(beta1⋆t))\"                            # bias-correct m\n",
    "E7 = \"v÷(1-(beta2⋆t))\"                            # bias-correct v\n",
    "E8 = \"p-(lr×mhat÷((√vhat)+eps))\"                  # param update\n",
    "\n",
    "print(\"Nine BQN expressions define the entire MLP + optimizer:\")\n",
    "for i, e in enumerate([E0,E1,E2,E3,E4,E5,E6,E7,E8]):\n",
    "    print(f\"  E{i} = {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60adf31",
   "metadata": {},
   "source": [
    "## Compile BQN → JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38decfc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BQNUnsupportedError",
     "evalue": "Unsupported in JAX IR backend: call forms beyond fold-call lowering",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBQNUnsupportedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m _ce_s = \u001b[38;5;28;01mlambda\u001b[39;00m expr, names: compile_expression(expr, arg_names=names, shape_policy=_STATIC)\n\u001b[32m      4\u001b[39m _ce_d = \u001b[38;5;28;01mlambda\u001b[39;00m expr, names: compile_expression(expr, arg_names=names, shape_policy=_DYNAMIC)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m F   = \u001b[43m_ce_d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m      7\u001b[39m YF  = _ce_d(E1, (\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m,))                        \u001b[38;5;66;03m# target\u001b[39;00m\n\u001b[32m      8\u001b[39m LF  = _ce_s(E2, (\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m))                     \u001b[38;5;66;03m# loss\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(expr, names)\u001b[39m\n\u001b[32m      2\u001b[39m _DYNAMIC = ShapePolicy(kind=\u001b[33m\"\u001b[39m\u001b[33mdynamic\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m _ce_s = \u001b[38;5;28;01mlambda\u001b[39;00m expr, names: compile_expression(expr, arg_names=names, shape_policy=_STATIC)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m _ce_d = \u001b[38;5;28;01mlambda\u001b[39;00m expr, names: \u001b[43mcompile_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_DYNAMIC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m F   = _ce_d(E0, (\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbo\u001b[39m\u001b[33m\"\u001b[39m))   \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m      7\u001b[39m YF  = _ce_d(E1, (\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m,))                        \u001b[38;5;66;03m# target\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/jax/xlapl/bqn-jax/src/bqn_jax/ir.py:1913\u001b[39m, in \u001b[36mcompile_expression\u001b[39m\u001b[34m(source, arg_names, constants, shape_policy, optimize, use_cache, trace, trace_args)\u001b[39m\n\u001b[32m   1898\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_expression\u001b[39m(\n\u001b[32m   1899\u001b[39m     source: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1900\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1907\u001b[39m     trace_args: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mobject\u001b[39m, ...] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1908\u001b[39m ):\n\u001b[32m   1909\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compile a numeric subset expression to IR-backed callable.\u001b[39;00m\n\u001b[32m   1910\u001b[39m \n\u001b[32m   1911\u001b[39m \u001b[33;03m    When `trace=True`, returns `(compiled, jaxpr)` and requires `trace_args`.\u001b[39;00m\n\u001b[32m   1912\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1913\u001b[39m     ir = \u001b[43mlower_to_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m     compiled = CompiledExpression(ir=ir, source=source, shape_policy=shape_policy \u001b[38;5;129;01mor\u001b[39;00m ShapePolicy())\n\u001b[32m   1921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trace:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/jax/xlapl/bqn-jax/src/bqn_jax/ir.py:1890\u001b[39m, in \u001b[36mlower_to_ir\u001b[39m\u001b[34m(source, arg_names, constants, optimize, use_cache)\u001b[39m\n\u001b[32m   1887\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BQNParseError.from_parse_error(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1889\u001b[39m lowerer = _Lowerer(arg_names=arg_names, constants=consts)\n\u001b[32m-> \u001b[39m\u001b[32m1890\u001b[39m out = \u001b[43mlowerer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1891\u001b[39m ir = JaxIR(nodes=\u001b[38;5;28mtuple\u001b[39m(lowerer.nodes), output=out, arg_names=\u001b[38;5;28mtuple\u001b[39m(arg_names))\n\u001b[32m   1892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/jax/xlapl/bqn-jax/src/bqn_jax/ir.py:743\u001b[39m, in \u001b[36m_Lowerer.lower_expr\u001b[39m\u001b[34m(self, expr)\u001b[39m\n\u001b[32m    740\u001b[39m         left_id = \u001b[38;5;28mself\u001b[39m.lower_expr(expr.left)\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_expr(expr, \u001b[38;5;28mself\u001b[39m._add(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfold_init:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_op\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, inputs=(left_id, right_id)))\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _unsupported_backend(\u001b[33m\"\u001b[39m\u001b[33mcall forms beyond fold-call lowering\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, Nothing):\n\u001b[32m    746\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _unsupported_runtime_only(\u001b[33m\"\u001b[39m\u001b[33mnothing literal ·\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mBQNUnsupportedError\u001b[39m: Unsupported in JAX IR backend: call forms beyond fold-call lowering"
     ]
    }
   ],
   "source": [
    "_STATIC  = ShapePolicy(kind=\"static\")\n",
    "_DYNAMIC = ShapePolicy(kind=\"dynamic\")\n",
    "_ce_s = lambda expr, names: compile_expression(expr, arg_names=names, shape_policy=_STATIC)\n",
    "_ce_d = lambda expr, names: compile_expression(expr, arg_names=names, shape_policy=_DYNAMIC)\n",
    "\n",
    "F   = _ce_d(E0, (\"x\", \"w\", \"b\", \"v\", \"bo\"))   # forward\n",
    "YF  = _ce_d(E1, (\"x\",))                        # target\n",
    "LF  = _ce_s(E2, (\"p\", \"y\"))                     # loss\n",
    "AF  = _ce_d(E3, (\"p\", \"y\"))                     # rmse\n",
    "MUF = _ce_s(E4, (\"m\", \"g\", \"beta1\"))            # Adam momentum\n",
    "VUF = _ce_s(E5, (\"v\", \"g\", \"beta2\"))            # Adam velocity\n",
    "MHF = _ce_s(E6, (\"m\", \"beta1\", \"t\"))            # bias-correct m\n",
    "VHF = _ce_s(E7, (\"v\", \"beta2\", \"t\"))            # bias-correct v\n",
    "PUF = _ce_s(E8, (\"p\", \"mhat\", \"vhat\", \"lr\", \"eps\"))  # param update\n",
    "\n",
    "# Vectorize over batch dimension\n",
    "Y_MAP    = YF.vmap(in_axes=0, out_axes=0)\n",
    "PRED_MAP = F.vmap(in_axes=(0, None, None, None, None), out_axes=0)\n",
    "\n",
    "print(\"All BQN expressions compiled to JAX successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed637e9",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "The target polynomial is:\n",
    "\n",
    "$$y = 0.2 + 0.9x - 0.3x^2 + 0.4x^3$$\n",
    "\n",
    "expressed in BQN as `E1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858de787",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "N_SAMPLES = 2048\n",
    "\n",
    "k = jax.random.PRNGKey(SEED)\n",
    "x_all = jax.random.uniform(k, (N_SAMPLES, 1), minval=-1.5, maxval=1.5, dtype=jnp.float32)\n",
    "y_all = Y_MAP(x_all).astype(jnp.float32)\n",
    "\n",
    "n = int(0.8 * N_SAMPLES)\n",
    "xt, yt = x_all[:n], y_all[:n]\n",
    "xv, yv = x_all[n:], y_all[n:]\n",
    "\n",
    "print(f\"Training: {xt.shape[0]} samples  |  Validation: {xv.shape[0]} samples\")\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "idx = np.argsort(np.array(xt[:, 0]))\n",
    "ax.scatter(np.array(xt[:, 0]), np.array(yt), s=4, alpha=0.3, label='Train')\n",
    "ax.scatter(np.array(xv[:, 0]), np.array(yv), s=4, alpha=0.3, color='orange', label='Val')\n",
    "\n",
    "xs = np.linspace(-1.5, 1.5, 300).reshape(-1, 1)\n",
    "ys = np.array(Y_MAP(jnp.array(xs, dtype=jnp.float32)))\n",
    "ax.plot(xs[:, 0], ys, 'k-', lw=2, label='Target polynomial')\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "ax.set_title('Target: $y = 0.2 + 0.9x - 0.3x^2 + 0.4x^3$')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef318ce6",
   "metadata": {},
   "source": [
    "## Initialize Weights & Adam State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2 = jax.random.split(jax.random.PRNGKey(SEED + 1))\n",
    "\n",
    "HIDDEN = 16\n",
    "\n",
    "# w: (1, 16), b: (16,), v: (16,), bo: scalar\n",
    "w = (\n",
    "    0.2 * jax.random.normal(k1, (1, HIDDEN), dtype=jnp.float32),   # input weights\n",
    "    jnp.zeros((HIDDEN,), dtype=jnp.float32),                       # hidden bias\n",
    "    0.2 * jax.random.normal(k2, (HIDDEN,), dtype=jnp.float32),     # output weights\n",
    "    jnp.asarray(0.0, dtype=jnp.float32),                           # output bias\n",
    ")\n",
    "\n",
    "# Adam state\n",
    "m_state = jax.tree_util.tree_map(jnp.zeros_like, w)\n",
    "v_state = jax.tree_util.tree_map(jnp.zeros_like, w)\n",
    "t = jnp.asarray(1.0, dtype=jnp.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "LR    = jnp.asarray(0.02, dtype=jnp.float32)\n",
    "BETA1 = jnp.asarray(0.9,  dtype=jnp.float32)\n",
    "BETA2 = jnp.asarray(0.999, dtype=jnp.float32)\n",
    "EPS   = jnp.asarray(1e-8, dtype=jnp.float32)\n",
    "\n",
    "print(f\"Architecture: 1 → {HIDDEN} (BQN ReLU-like) → 1\")\n",
    "n_params = sum(p.size for p in jax.tree_util.tree_leaves(w))\n",
    "print(f\"Parameters: {n_params}\")\n",
    "print(f\"Optimizer: Adam (lr={float(LR)}, β₁={float(BETA1)}, β₂={float(BETA2)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129d448",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(q, xb):\n",
    "    return PRED_MAP(xb, q[0], q[1], q[2], q[3])\n",
    "\n",
    "def loss(q, xb, yb):\n",
    "    return LF(pred(q, xb), yb)\n",
    "\n",
    "@jax.jit\n",
    "def step(q, m_acc, v_acc, step_t, xb, yb):\n",
    "    l, grads = jax.value_and_grad(loss)(q, xb, yb)\n",
    "    m_acc = jax.tree_util.tree_map(lambda m, g: MUF(m, g, BETA1), m_acc, grads)\n",
    "    v_acc = jax.tree_util.tree_map(lambda v, g: VUF(v, g, BETA2), v_acc, grads)\n",
    "    m_hat = jax.tree_util.tree_map(lambda m: MHF(m, BETA1, step_t), m_acc)\n",
    "    v_hat = jax.tree_util.tree_map(lambda v: VHF(v, BETA2, step_t), v_acc)\n",
    "    q = jax.tree_util.tree_map(lambda p, mh, vh: PUF(p, mh, vh, LR, EPS), q, m_hat, v_hat)\n",
    "    return q, m_acc, v_acc, step_t + 1.0, l\n",
    "\n",
    "def rmse(q, xb, yb):\n",
    "    return float(AF(pred(q, xb), yb))\n",
    "\n",
    "EPOCHS = 1000\n",
    "LOG_EVERY = 50\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_rmses, val_rmses = [], []\n",
    "snapshots = {}  # epoch -> prediction on grid\n",
    "\n",
    "xs_grid = jnp.linspace(-1.5, 1.5, 300).reshape(-1, 1).astype(jnp.float32)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    w, m_state, v_state, t, l = step(w, m_state, v_state, t, xt, yt)\n",
    "    tl = float(l)\n",
    "    vl = float(loss(w, xv, yv))\n",
    "    train_losses.append(tl)\n",
    "    val_losses.append(vl)\n",
    "\n",
    "    if e % LOG_EVERY == 0 or e == EPOCHS - 1:\n",
    "        tr = rmse(w, xt, yt)\n",
    "        vr = rmse(w, xv, yv)\n",
    "        train_rmses.append(tr)\n",
    "        val_rmses.append(vr)\n",
    "        snapshots[e] = np.array(pred(w, xs_grid))\n",
    "        print(f\"epoch {e:4d}  loss={tl:.6f}  train_rmse={tr:.4f}  val_rmse={vr:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef10fb",
   "metadata": {},
   "source": [
    "## Loss & RMSE Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Train MSE', color='#2563eb', lw=1.2)\n",
    "ax1.plot(val_losses, label='Val MSE', color='#dc2626', lw=1.2, alpha=0.7)\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('MSE')\n",
    "ax1.set_title('Loss Curves')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "rmse_epochs = sorted(snapshots.keys())\n",
    "ax2.plot(rmse_epochs, train_rmses, 'o-', ms=4, label='Train RMSE', color='#2563eb')\n",
    "ax2.plot(rmse_epochs, val_rmses, 's-', ms=4, label='Val RMSE', color='#dc2626')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('RMSE Over Training')\n",
    "ax2.legend(); ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b817fe7",
   "metadata": {},
   "source": [
    "## MLP Fit Progression\n",
    "\n",
    "Watch the network learn the polynomial shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d52e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true = np.array(Y_MAP(xs_grid))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.scatter(np.array(xt[:, 0]), np.array(yt), s=4, alpha=0.15, color='gray', label='Train data')\n",
    "ax.plot(np.array(xs_grid[:, 0]), ys_true, 'k--', lw=2, alpha=0.6, label='Target polynomial')\n",
    "\n",
    "colors = plt.cm.plasma(np.linspace(0.1, 0.9, len(snapshots)))\n",
    "for (ep, y_pred), color in zip(sorted(snapshots.items()), colors):\n",
    "    ax.plot(np.array(xs_grid[:, 0]), y_pred, color=color, lw=1.5, alpha=0.8, label=f'Epoch {ep}')\n",
    "\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "ax.set_title('MLP Fit Progression — BQN Forward Pass + Adam Optimizer')\n",
    "ax.legend(fontsize=7, ncol=2, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc24ce",
   "metadata": {},
   "source": [
    "## Final Prediction vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31692758",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = np.array(pred(w, xs_grid))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overlay\n",
    "ax1.scatter(np.array(xv[:, 0]), np.array(yv), s=10, alpha=0.4, color='steelblue', label='Val data')\n",
    "ax1.plot(np.array(xs_grid[:, 0]), ys_true, 'k-', lw=2, label='Target')\n",
    "ax1.plot(np.array(xs_grid[:, 0]), y_final, 'r-', lw=2.5, label='MLP prediction')\n",
    "ax1.fill_between(np.array(xs_grid[:, 0]), ys_true.ravel(), y_final.ravel(),\n",
    "                  alpha=0.15, color='red')\n",
    "final_vr = rmse(w, xv, yv)\n",
    "ax1.set_title(f'Final Fit — Val RMSE = {final_vr:.4f}')\n",
    "ax1.set_xlabel('x'); ax1.set_ylabel('y')\n",
    "ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_final.ravel() - ys_true.ravel()\n",
    "ax2.plot(np.array(xs_grid[:, 0]), residuals, 'r-', lw=1.5)\n",
    "ax2.axhline(0, color='k', ls='--', lw=1)\n",
    "ax2.fill_between(np.array(xs_grid[:, 0]), residuals, alpha=0.2, color='red')\n",
    "ax2.set_xlabel('x'); ax2.set_ylabel('Residual')\n",
    "ax2.set_title('Residuals (MLP − Target)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f0c22",
   "metadata": {},
   "source": [
    "## Hidden Unit Activations\n",
    "\n",
    "Peek inside the trained network to see what each of the 16 hidden units learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17657b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pre- and post-activation for each hidden unit\n",
    "W_in, bias, W_out, b_out = w\n",
    "pre_act = np.array(xs_grid) @ np.array(W_in).T + np.array(bias)  # (300, 16)\n",
    "post_act = pre_act / (1 + np.abs(pre_act))  # BQN's ÷⟜(1˙⊸+∘|)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 10), sharex=True)\n",
    "x_np = np.array(xs_grid[:, 0])\n",
    "w_out_np = np.array(W_out)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    color = 'tab:blue' if w_out_np[i] >= 0 else 'tab:red'\n",
    "    ax.plot(x_np, post_act[:, i], color=color, lw=1.5)\n",
    "    ax.axhline(0, color='gray', ls=':', lw=0.5)\n",
    "    ax.set_title(f'Unit {i} (w={w_out_np[i]:.2f})', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "fig.suptitle('Hidden Unit Activations (blue=positive output weight, red=negative)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc4da",
   "metadata": {},
   "source": [
    "## Weight Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = np.concatenate([np.array(p).ravel() for p in jax.tree_util.tree_leaves(w)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(all_params, bins=40, color='#6366f1', edgecolor='white', alpha=0.8)\n",
    "ax.axvline(0, color='k', ls='--', lw=1)\n",
    "ax.set_xlabel('Weight value'); ax.set_ylabel('Count')\n",
    "ax.set_title(f'Weight Distribution ({len(all_params)} parameters)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary\")\n",
    "print(f\"  Nine BQN expressions compiled to JAX\")\n",
    "print(f\"  {len(all_params)} trainable parameters\")\n",
    "print(f\"  Final val RMSE: {rmse(w, xv, yv):.4f}\")\n",
    "print(f\"  All math expressed in BQN, differentiated and JIT-compiled by JAX.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
